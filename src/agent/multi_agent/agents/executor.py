"""
Executor æ™ºèƒ½ä½“ - æ‰§è¡Œè€…

è´Ÿè´£æ‰§è¡Œå…·ä½“ä»»åŠ¡ï¼Œå¦‚æœç´¢ä¿¡æ¯ã€ç”Ÿæˆå†…å®¹ç­‰
"""
from typing import Dict, Any, List
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain.tools import BaseTool
from src.config import settings
from src.utils import app_logger
from src.agent.multi_agent.chat_state import ChatState


class ExecutorAgent:
    """
    æ‰§è¡Œè€…æ™ºèƒ½ä½“

    èŒè´£ï¼š
    1. æ¥æ”¶ Planner çš„æ‰§è¡ŒæŒ‡ä»¤
    2. è°ƒç”¨å·¥å…·å®Œæˆå…·ä½“ä»»åŠ¡
    3. ç”Ÿæˆç»“æ„åŒ–çš„å“åº”
    """

    def __init__(self, llm: ChatOpenAI = None, tools: List[BaseTool] = None):
        """åˆå§‹åŒ–æ‰§è¡Œè€…"""
        self.name = "Executor"
        self.llm = llm or ChatOpenAI(
            model=settings.model_name,
            temperature=0.7,  # é€‚ä¸­æ¸©åº¦ï¼Œä¿æŒåˆ›é€ æ€§
            max_tokens=settings.max_tokens,
            openai_api_key=settings.openai_api_key,
            openai_api_base=settings.openai_api_base,
        )

        self.tools = tools or []
        self.tool_map = {tool.name: tool for tool in self.tools}

        # å¦‚æœæœ‰å·¥å…·ï¼Œç»‘å®šåˆ° LLM
        if self.tools:
            self.llm_with_tools = self.llm.bind_tools(self.tools)
        else:
            self.llm_with_tools = self.llm

        self.system_prompt = self._get_system_prompt()

        app_logger.info(f"[{self.name}] åˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨å·¥å…·: {list(self.tool_map.keys())}")

    def _get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        tool_descriptions = ""
        if self.tools:
            tool_descriptions = "\nã€å¯ç”¨å·¥å…·ã€‘\n"
            for tool in self.tools:
                tool_descriptions += f"- {tool.name}: {tool.description}\n"

        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ‰§è¡Œè€…ï¼ˆExecutorï¼‰ï¼Œè´Ÿè´£æ‰§è¡Œå…·ä½“ä»»åŠ¡ã€‚

ã€æ ¸å¿ƒèŒè´£ã€‘
1. ç†è§£è§„åˆ’è€…ï¼ˆPlannerï¼‰ç»™å‡ºçš„æ‰§è¡ŒæŒ‡ä»¤
2. è°ƒç”¨åˆé€‚çš„å·¥å…·å®Œæˆä»»åŠ¡
3. ç”Ÿæˆæ¸…æ™°ã€å‡†ç¡®çš„å“åº”

{tool_descriptions}

ã€å·¥ä½œæµç¨‹ã€‘
1. ä»”ç»†é˜…è¯»æ‰§è¡ŒæŒ‡ä»¤
2. æŸ¥çœ‹å¯¹è¯å†å²ï¼Œç†è§£ä¸Šä¸‹æ–‡
3. å¦‚æœéœ€è¦ï¼Œè°ƒç”¨å·¥å…·è·å–ä¿¡æ¯
4. æ•´ç†ä¿¡æ¯ï¼Œç”Ÿæˆä¸“ä¸šçš„å›ç­”
5. ç¡®ä¿å›ç­”å‡†ç¡®ã€å®Œæ•´ã€æ˜“æ‡‚

ã€å›ç­”è¦æ±‚ã€‘
1. å‡†ç¡®æ€§ï¼šç¡®ä¿ä¿¡æ¯å‡†ç¡®æ— è¯¯
2. å®Œæ•´æ€§ï¼šå…¨é¢å›ç­”ç”¨æˆ·é—®é¢˜
3. æ¸…æ™°æ€§ï¼šä½¿ç”¨æ¸…æ™°çš„è¯­è¨€å’Œç»“æ„
4. ä¸“ä¸šæ€§ï¼šä¿æŒä¸“ä¸šçš„æ€åº¦
5. å‹å¥½æ€§ï¼šè¯­æ°”å‹å¥½ã€ä¹äºåŠ©äºº

ã€æ³¨æ„äº‹é¡¹ã€‘
1. å¦‚æœå·¥å…·è¿”å›çš„ä¿¡æ¯ä¸è¶³ï¼Œè¯´æ˜æƒ…å†µ
2. å¦‚æœæ— æ³•å®Œæˆä»»åŠ¡ï¼Œè¯šå®å‘ŠçŸ¥ç”¨æˆ·
3. å¼•ç”¨çŸ¥è¯†åº“ä¿¡æ¯æ—¶è¯´æ˜æ¥æº
4. é€‚å½“ä½¿ç”¨æ ¼å¼åŒ–ï¼ˆå¦‚åˆ—è¡¨ã€æ­¥éª¤ç­‰ï¼‰æé«˜å¯è¯»æ€§
5. ä½¿ç”¨å·¥å…·æ—¶è¯´æ˜é€‰æ‹©ç†ç”±
"""

    async def execute(self, state: ChatState) -> Dict[str, Any]:
        """
        æ‰§è¡Œä»»åŠ¡

        Args:
            state: å½“å‰èŠå¤©çŠ¶æ€

        Returns:
            æ›´æ–°åçš„çŠ¶æ€å­—æ®µ
        """
        app_logger.info(f"[{self.name}] å¼€å§‹æ‰§è¡Œä»»åŠ¡...")

        # è·å–æ‰§è¡ŒæŒ‡ä»¤
        instruction = state.get("execution_instruction", "")
        if not instruction:
            app_logger.warning(f"[{self.name}] æœªæ”¶åˆ°æ‰§è¡ŒæŒ‡ä»¤")
            return {}

        app_logger.info(f"[{self.name}] æ‰§è¡ŒæŒ‡ä»¤: {instruction[:100]}...")

        # è·å–æ¶ˆæ¯å†å²
        messages = state["messages"]

        # æ„å»ºæç¤º
        prompt_messages = [
            SystemMessage(content=self.system_prompt),
        ]

        # æ·»åŠ å¯¹è¯å†å²ï¼ˆæœ€è¿‘10æ¡ï¼‰
        recent_messages = messages[-10:] if len(messages) > 10 else messages
        prompt_messages.extend(recent_messages)

        # æ·»åŠ æ‰§è¡ŒæŒ‡ä»¤
        prompt_messages.append(
            HumanMessage(content=f"ã€æ‰§è¡ŒæŒ‡ä»¤ã€‘\n{instruction}")
        )

        # è®°å½•æç¤º
        self._log_prompt(prompt_messages)

        # è°ƒç”¨ LLMï¼ˆå¯èƒ½ä¼šè°ƒç”¨å·¥å…·ï¼‰
        try:
            response = await self.llm_with_tools.ainvoke(prompt_messages)

            # å¤„ç†å·¥å…·è°ƒç”¨
            if hasattr(response, 'tool_calls') and response.tool_calls:
                app_logger.info(f"[{self.name}] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {len(response.tool_calls)} ä¸ª")

                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = []
                for tool_call in response.tool_calls:
                    tool_name = tool_call["name"]
                    tool_args = tool_call["args"]

                    app_logger.info(f"[{self.name}] è°ƒç”¨å·¥å…·: {tool_name}")
                    app_logger.debug(f"[{self.name}] å·¥å…·å‚æ•°: {tool_args}")

                    if tool_name in self.tool_map:
                        tool = self.tool_map[tool_name]
                        try:
                            result = await tool.ainvoke(tool_args)
                            tool_results.append({
                                "tool": tool_name,
                                "result": result
                            })
                            app_logger.info(f"[{self.name}] å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸ")
                        except Exception as e:
                            app_logger.error(f"[{self.name}] å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {e}")
                            tool_results.append({
                                "tool": tool_name,
                                "error": str(e)
                            })
                    else:
                        app_logger.warning(f"[{self.name}] æœªæ‰¾åˆ°å·¥å…·: {tool_name}")

                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œå†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”
                prompt_messages.append(response)

                # æ·»åŠ å·¥å…·ç»“æœ
                from langchain_core.messages import ToolMessage
                for i, tool_result in enumerate(tool_results):
                    tool_msg = ToolMessage(
                        content=str(tool_result.get("result", tool_result.get("error", ""))),
                        tool_call_id=response.tool_calls[i]["id"]
                    )
                    prompt_messages.append(tool_msg)

                # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå“åº”
                final_response = await self.llm.ainvoke(prompt_messages)
                response_text = final_response.content
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨å“åº”
                response_text = response.content

            # è®°å½•å“åº”
            self._log_response(response_text)

            app_logger.info(f"[{self.name}] ä»»åŠ¡æ‰§è¡Œå®Œæˆ")

            # å°†å“åº”æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            return {
                "messages": [AIMessage(content=response_text)]
            }

        except Exception as e:
            app_logger.error(f"[{self.name}] æ‰§è¡Œå¤±è´¥: {e}")
            error_msg = f"æŠ±æ­‰ï¼Œæ‰§è¡Œä»»åŠ¡æ—¶é‡åˆ°é”™è¯¯: {str(e)}"
            return {
                "messages": [AIMessage(content=error_msg)]
            }

    def _log_prompt(self, messages):
        """è®°å½•æç¤º"""
        app_logger.info(f"[{self.name}] ğŸ“¤ å‘é€æç¤º (æ¶ˆæ¯æ•°: {len(messages)})")
        for i, msg in enumerate(messages):
            msg_type = msg.__class__.__name__
            content_preview = msg.content[:100] if len(msg.content) > 100 else msg.content
            app_logger.debug(f"  [{i+1}] {msg_type}: {content_preview}...")

    def _log_response(self, response: str):
        """è®°å½•å“åº”"""
        preview = response[:200] if len(response) > 200 else response
        app_logger.info(f"[{self.name}] ğŸ“¥ æ”¶åˆ°å“åº”: {preview}...")

